---
title: Create a Voice Virtual Assistant with ElevenLabs
author: Alexandre Sajus
uid: u2DitJXVOqWo18bLsyXNLcVtufC2
datePublished: 2025-04-22
description: Learn how to build a Voice Virtual Assistant using ElevenLabs API and Python for seamless AI-powered interactions.
published: live
header: https://raw.githubusercontent.com/codedex-io/projects/main/projects/create-a-voice-virtual-assistant-with-elevenlabs/header.gif
bannerImage: https://raw.githubusercontent.com/codedex-io/projects/main/projects/create-a-voice-virtual-assistant-with-elevenlabs/header.gif
readTime: 60
prerequisites: Python fundamentals, API usage
versions: Python 3.11, python-dotenv 1.0.1, elevenlabs 1.54.0, elevenlabs[pyaudio]
courses:
  - python
  - command-line
tags:
  - intermediate
  - python
  - AI
---

## Introduction

Hey! üëã I'm Alex! Since this is my first project tutorial on Cod√©dex, I wanted to revisit a very popular project I made in 2023: creating a voice virtual assistant using Python.

Back in 2023, I had just graduated with a Master's of Science in Artificial Intelligence and I had already made a few fun AI projects like detecting vehicles in video games using object detection or teaching 3D characters to jump over obstacles using reinforcement learning (You can check them out on my [YouTube channel](https://www.youtube.com/@alexandresajus/videos)).

<ImageZoom src="https://i.imgur.com/W5fyBFH.png" style={{width: "60%", height: "auto"}}/>

AI around language was getting more and more popular: ChatGPT released in 2022, and multiple companies were releasing APIs around managing text transcription and voice synthesis. 

Despite this, voice-based conversational AI was still in its early stages and voice assistants like the one in ChatGPT were just starting to be released in beta.

That's why I decided to create my own! Back then, APIs were limited in functionality so I had to build my own pipeline with one tool per task:
- üé§ PyAudio to record the user's voice
- ‚å®Ô∏è Deepgram to transcribe the voice to text
- ü§ñ OpenAI GPT-3 to generate a response
- üìà ElevenLabs to convert the response to speech
- üîä Pygame to play the response
- üíª Taipy to display the conversation
- ü§ù And a lot of Python code to glue everything together

<ImageZoom src="https://i.imgur.com/bWX2sx8.png" />

It was a lot of work but it was worth it! I released it on [GitHub](https://github.com/AlexandreSajus/JARVIS) and on [YouTube](https://github.com/AlexandreSajus/JARVIS) and it got 24k views, 485 stars, and 88 forks! Here's me talking to my chatbot after spending 24 hours without sleep creating it:

<ImageZoom src="https://i.imgur.com/ecC0Tff.png" />

Now, in 2025, I'm excited to revisit this project since APIs have evolved a lot since then. What took 6 different libraries to work in 2023 can now be done with just one API. In this tutorial, we will use the ElevenLabs API to record our voice and play the assistant's response in real time:

<video
  controls
  src="https://i.imgur.com/Rhcgc2O.mp4"
  width="620">
</video>

Let's dive in!

## Setting Up the Environment

### Install Required Packages

Before we start, make sure you have Python installed. Then, install the required dependencies:

```sh
pip install elevenlabs elevenlabs[pyaudio] python-dotenv
```

Processing audio requires additional dependencies on Linux and MacOS:

- For Linux, you need to install `portaudio19`:
```sh
sudo apt install portaudio19
```
- For MacOS, you need to install `portaudio`:
```sh
brew install portaudio
```

### Setting up ElevenLabs

ElevenLabs provides a Conversational AI API that we will use to create our Voice Assistant. 
- üé§ The API records the user's voice through the microphone
- üñ®Ô∏è It processes it to know when the user has finished speaking or is interrupting the assistant
- ü§ñ It calls an LLM model to generate a response
- üìà It synthesizes the response into speech
- üîä It plays the synthesized speech through the speakers

<ImageZoom src="https://i.imgur.com/QZkz0Rh.png" />

1. Sign up at [ElevenLabs](https://elevenlabs.io/app/sign-up) and follow the instructions to create an account.

2. Once signed in, go to "Conversational AI".

<ImageZoom src="https://i.imgur.com/aIYfusq.png" />

3. Go to "Agents".

<ImageZoom src="https://i.imgur.com/L9xwBgl.png" />

4. Click on "Start from blank".

<ImageZoom src="https://i.imgur.com/PD8v3Ax.png" style={{width: "60%", height: "auto"}}/>

5. Create a **.env** file at the root of your project folder. We will use this file to store our API credentials securely. This way they won't be hardcoded in the script. In this **.env** file, add your Agent ID:

<ImageZoom src="https://i.imgur.com/vfmMv7r.png"/>

```bash
AGENT_ID=your_agent_id
```

There shouldn‚Äôt be spaces around the `=` in a **.env** file.

6. Go to the "Security" tab, enable the "First message" and "System prompt" overrides, and save. This will allow us to customize the assistant's first message and system prompt using Python code.

<ImageZoom src="https://i.imgur.com/0vfNTOd.png"/>

7. Click on your profile and go to "API keys". Create a new API key and copy it to your **.env** file:

```bash
API_KEY="sk_XXX...XXX"
```

**Important:** Make sure to save your **.env** file after adding the credentials.

<ImageZoom src="https://i.imgur.com/Q5QrGVl.png" />

ElevenLabs is now set up and ready to be used in our Python script!

**Note:** ElevenLabs works with a credit system. When you sign up, you get 10,000 free credits which amount to 15 minutes of conversation. You can buy more credits if needed.

## Building the Voice Assistant

### Load Environment Variables

Create a Python file (e.g., **voice_assistant.py**) and load your API credentials:

```py
import os
from dotenv import load_dotenv

load_dotenv()

AGENT_ID = os.getenv("AGENT_ID")
API_KEY = os.getenv("API_KEY")
```

### 2. Configure ElevenLabs Conversation API

We will set up the ElevenLabs client and configure a conversation instance.

We'll start by importing the necessary modules:

```py
from elevenlabs.client import ElevenLabs
from elevenlabs.conversational_ai.conversation import Conversation
from elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface
from elevenlabs.types import ConversationConfig
```

We will then configure the conversation with the agent's first message and system prompt. 

We are going to inform the assistant that the user has a schedule and prompt it to help the user. In this part you can customize:

- The **user's name**: what the assistant will call the user
- The **schedule**: the user's schedule that the assistant will use to provide help
- The **prompt**: the message that the assistant will receive when the conversation starts to understand the context of the conversation
- The **first message**: the first message the assistant will say to the user

**Prompts** are used to provide context to the assistant and help it understand the user's needs. 

Here's my example:

```py
user_name = "Alex"
schedule = "Sales Meeting with Taipy at 10:00; Gym with Sophie at 17:00"
prompt = f"You are a helpful assistant. Your interlocutor has the following schedule: {schedule}."
first_message = f"Hello {user_name}, how can I help you today?"
```

Underneath in the same file, we are then going to set this configuration to our ElevenLabs agent:

```py
conversation_override = {
  "agent": {
    "prompt": {
      "prompt": prompt,
    },
    "first_message": first_message,
  },
}

config = ConversationConfig(
  conversation_config_override=conversation_override,
  extra_body={},
  dynamic_variables={},
)

client = ElevenLabs(api_key=API_KEY)

conversation = Conversation(
  client,
  AGENT_ID,
  config=config,
  requires_auth=True,
  audio_interface=DefaultAudioInterface(),
)
```

### 3. Implement Callbacks for Responses

We'll also need to handle assistant responses by printing the assistant's responses and user transcripts, as well as handling the situation where the user interrupts the assistant. We can do so by implementing a few callback functions underneath our configuration.

```py
def print_agent_response(response):
  print(f"Agent: {response}")

def print_interrupted_response(original, corrected):
  print(f"Agent interrupted, truncated response: {corrected}")

def print_user_transcript(transcript):
  print(f"User: {transcript}")
```

### 4. Start the Voice Assistant Session

Finally, initiate the conversation session in the same file:

```py
conversation = Conversation(
  client,
  AGENT_ID,
  config=config,
  requires_auth=True,
  audio_interface=DefaultAudioInterface(),
  callback_agent_response=print_agent_response,
  callback_agent_response_correction=print_interrupted_response,
  callback_user_transcript=print_user_transcript,
)

conversation.start_session()
```

## Running the Assistant

**Please make sure your audio devices are correctly set up in your system settings before running the code.**

Execute the script:

```bash
python voice_assistant.py
```

The assistant will start listening for input and responding in real time!

You can stop the assistant at any time by closing the terminal.

## Conclusion

Congratulations! üéâ 

You've successfully built a Voice Virtual Assistant using ElevenLabs API. You can extend its capabilities by integrating it with home automation, calendars, or other APIs to make it even more useful.

Stay creative and keep experimenting with AI-powered assistants!

### More Resources

- [Source Code](https://github.com/AlexandreSajus/codedex-voice/blob/main/voice_assistant.py)
- ElevenLabs Conversational AI [Overview](https://elevenlabs.io/docs/conversational-ai/overview)
- ElevenLabs Python SDK [Documentation](https://elevenlabs.io/docs/conversational-ai/libraries/python)
- Enable your assistant to execute Python functions with [Client Tools](https://elevenlabs.io/docs/conversational-ai/customization/tools-events/client-tools)
- Provide documents as context to your assistant with [RAG](https://elevenlabs.io/docs/conversational-ai/customization/knowledge-base/rag)
